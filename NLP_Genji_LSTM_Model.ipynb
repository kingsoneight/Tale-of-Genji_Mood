{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyWb3x74dch7",
        "outputId": "cb3a9bf1-2316-4f17-facd-050e9122f45d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "81B7sXDRdkdr"
      },
      "outputs": [],
      "source": [
        "def read_file(path):\n",
        "  # Read file into an array of strings\n",
        "  lines = []\n",
        "  with open(path, encoding=\"utf8\") as file:\n",
        "      lines = file.readlines()\n",
        "  # Remove newline characters from each line\n",
        "  lines = [line.strip() for line in lines]\n",
        "  return lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keAJbaG5dleQ",
        "outputId": "d7e7efa1-7daf-499d-975c-cfda16cbe13b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1',\n",
              " '',\n",
              " 'KIRITSUBO',\n",
              " '',\n",
              " \"In a certain reign (whose can it have been?) someone of no very great rank, among all His Majesty's Consorts and Intimates, enjoyed exceptional favor. Those others who had always assumed that pride of place was properly theirs despised her as a dreadful woman, while the lesser Intimates were unhappier still. The way she waited on him day after day only stirred up feeling against her, and perhaps this growing burden of resentment was what affected her health and obliged her often to withdraw in misery to her home; but His Majesty, who could less and less do without her, ignored his critics until his behavior seemed bound to be the talk of all.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = \"/content/drive/My Drive/The Tale of Genji (Royall Tyler Translation)(Cleaned - With Titles).txt\"\n",
        "\n",
        "lines = read_file(data_dir)\n",
        "lines[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmgtRtg3gskc",
        "outputId": "c6ada8fc-8442-4590-faab-a0776d0f8232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['KIRITSUBO', '', \"In a certain reign (whose can it have been?) someone of no very great rank, among all His Majesty's Consorts and Intimates, enjoyed exceptional favor. Those others who had always assumed that pride of place was properly theirs despised her as a dreadful woman, while the lesser Intimates were unhappier still. The way she waited on him day after day only stirred up feeling against her, and perhaps this growing burden of resentment was what affected her health and obliged her often to withdraw in misery to her home; but His Majesty, who could less and less do without her, ignored his critics until his behavior seemed bound to be the talk of all.\", '', 'From this sad spectacle the senior nobles and privy gentlemen could only avert their eyes. Such things had led to disorder and ruin even in China, they said, and as discontent spread through the realm, the example of Yōkihi came more and more to mind, with many a painful consequence for the lady herself; yet she trusted in his gracious and unexampled affection and remained at court.']\n"
          ]
        }
      ],
      "source": [
        "def split_into_chapters(lines):\n",
        "    chapters = []\n",
        "    current_chapter = []\n",
        "    skip_next_line = False\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip().isdigit():\n",
        "            # Start of a new chapter\n",
        "            if current_chapter:\n",
        "                chapters.append(current_chapter)\n",
        "                current_chapter = []\n",
        "            skip_next_line = True  # Skip the next line (chapter title)\n",
        "        elif skip_next_line:\n",
        "            # Skip this line (chapter title) and continue to the next line\n",
        "            skip_next_line = False\n",
        "            continue\n",
        "        else:\n",
        "            current_chapter.append(line)\n",
        "\n",
        "    if current_chapter:\n",
        "        chapters.append(current_chapter)\n",
        "\n",
        "    return chapters\n",
        "\n",
        "# Read file and split into chapters\n",
        "lines = read_file(data_dir)\n",
        "chapters = split_into_chapters(lines)\n",
        "\n",
        "# Example: print the first few lines of the first chapter's content\n",
        "print(chapters[0][:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIHw6p1jzUBr",
        "outputId": "f290a0e9-f1d5-4801-b867-f035fcdab1a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.23.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.16.0)\n",
            "Installing collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEnODNyvjz1b",
        "outputId": "80a7ae84-fcb5-42f2-ee97-9a707936df6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 8.832297597612653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Average Loss: 8.26039184842791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Average Loss: 7.047681263514927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Average Loss: 5.759328842163086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Average Loss: 4.741016796657017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Average Loss: 4.298915454319546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Average Loss: 4.190052645547049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Average Loss: 4.024015154157366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Average Loss: 3.946128708975656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Average Loss: 3.8000244072505405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # Progress bar\n",
        "\n",
        "# Assuming 'chapters' is a list of text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(chapters)\n",
        "sequences = []\n",
        "\n",
        "for chapter in chapters:\n",
        "    encoded = tokenizer.texts_to_sequences([chapter])[0]\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in sequences])\n",
        "sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Preparing data\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "y = torch.tensor(y, dtype=torch.long)  # Ensure y is of type Long\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# PyTorch dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "dataset = TextDataset(X, y)\n",
        "train_loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "# PyTorch LSTM model with dropout\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(x)\n",
        "        out = self.fc(output[:, -1, :])\n",
        "        return out\n",
        "\n",
        "model = LSTMModel(vocab_size, embedding_dim=50, hidden_dim=100)\n",
        "# model = model.to('cuda')  # Move model to GPU\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training loop with progress bar and detailed logging\n",
        "# for epoch in range(100): # would be 100\n",
        "#     total_loss = 0\n",
        "#     progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{100}', leave=False)\n",
        "\n",
        "#     for inputs, targets in progress_bar:\n",
        "#         # inputs, targets = inputs.to('cuda'), targets.to('cuda')  # Move data to GPU\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "#         progress_bar.set_postfix(loss=total_loss / len(train_loader))\n",
        "\n",
        "#     print(f'Epoch {epoch + 1}, Average Loss: {total_loss / len(train_loader)}')\n",
        "# Reduced number of epochs and dataset size\n",
        "num_epochs = 10\n",
        "subset_size = int(0.1 * len(dataset))  # 10% of the dataset\n",
        "subset_indices = np.random.choice(range(len(dataset)), subset_size, replace=False)\n",
        "subset = torch.utils.data.Subset(dataset, subset_indices)\n",
        "train_loader = DataLoader(subset, batch_size=256, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for inputs, targets in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Average Loss: {total_loss / len(train_loader)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_JUb4Loy3SyT"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(input_text, num_generated_words, model, tokenizer, max_sequence_len, temperature=1.0):\n",
        "    model.eval()\n",
        "    for _ in range(num_generated_words):\n",
        "        token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        input_tensor = torch.tensor(token_list, dtype=torch.long)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(input_tensor)\n",
        "            predictions = F.softmax(predictions / temperature, dim=-1)  # Apply temperature scaling\n",
        "            predicted_index = torch.multinomial(predictions, 1)[-1, 0].item()  # Sample from the distribution\n",
        "\n",
        "        output_word = tokenizer.index_word[predicted_index] if predicted_index in tokenizer.index_word else ''\n",
        "        input_text += \" \" + output_word if output_word else ''\n",
        "\n",
        "    return input_text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TpBs-EOrqdx",
        "outputId": "d7b58256-9691-4794-9e6f-d471dba83582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a certain reign (whose can it have been?) someone of no very great rank, among all His Majesty's Consorts and Intimates, enjoyed exceptional favor. “for some years now i have inhabited the same world as you and yet felt somehow quite different from before, which is why i have neither written, except as necessary, nor sought to learn your news. letters in kana take me time to read, and moments spent otherwise than calling the name are moments lost. that is why i have sent you nothing. i gather that your daughter is now with the heir apparent and that she has borne him a son. that is a very great joy. i say that because although i am only a mountain ascetic and desire no worldly glory, i must confess that i have for many years thought of nothing but you, even during my day and night devotions, and that my prayers have been for you, to the neglect of any longing of mine for the dew on the lotus. these autumn leaves from my home, carried to you on the wind.” “o seer who roams the vastness of the heavens, go and find for me he spent a day filled with memories reproaching her for refusing to forgive him, and after that he could not go there again. instead he sent a note to the main house: “the snow this morning left me feeling too unwell, and i am lazily enjoying the comforts of home.” his highness had never paid any particular attention to jijū before, but the intimacy of their shared grief moved him to say, “come into service here! it is not as though you were nothing to her highness.”\n"
          ]
        }
      ],
      "source": [
        "input_text = \"In a certain reign (whose can it have been?) someone of no very great rank, among all His Majesty's Consorts and Intimates, enjoyed exceptional favor.\"\n",
        "num_generated_words = 5\n",
        "temperature = 15.0  # Adjust this to see different outputs\n",
        "generated_text = generate_text(input_text, num_generated_words, model, tokenizer, max_sequence_len, temperature)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-TBe3KztKOV",
        "outputId": "1259a7c8-0584-4085-9217-9cabe265cfb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repeated sentence found: letters in kana take me time to read and moments spent otherwise than calling the name are moments lost\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by lowering the case and removing punctuation.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def check_repetition(generated_text, original_texts):\n",
        "    \"\"\"\n",
        "    Check if any sentence in the generated text exists in the original texts.\n",
        "\n",
        "    :param generated_text: The text generated by the model.\n",
        "    :param original_texts: List of original texts (each text can be a string or a list of strings).\n",
        "    :return: A repeated sentence if found, else None.\n",
        "    \"\"\"\n",
        "    # Preprocess the generated text\n",
        "    generated_sentences = [preprocess_text(sentence) for sentence in generated_text.split('. ')]\n",
        "\n",
        "    # Preprocess and split the original texts into sentences\n",
        "    original_sentences = set()\n",
        "    for text in original_texts:\n",
        "        # Join the text if it's a list of words/sentences\n",
        "        if isinstance(text, list):\n",
        "            text = ' '.join(text)\n",
        "\n",
        "        for sentence in text.split('. '):\n",
        "            original_sentences.add(preprocess_text(sentence))\n",
        "\n",
        "    # Check for repetition\n",
        "    for sentence in generated_sentences:\n",
        "        if sentence in original_sentences:\n",
        "            return sentence  # Return the first repeated sentence found\n",
        "\n",
        "    return None  # No repetition found\n",
        "\n",
        "# Example usage\n",
        "repeated_sentence = check_repetition(generated_text, chapters)\n",
        "if repeated_sentence:\n",
        "    print(f\"Repeated sentence found: {repeated_sentence}\")\n",
        "else:\n",
        "    print(\"No repetition found in the generated text.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"repeated_text_overall_percent: {len(repeated_sentence) / len(generated_text)} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNZfjgd2oUGy",
        "outputId": "c1e01d05-e690-4382-e67b-da156b2919b2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "repeated_text_over_generated_text: 0.0682571239231279 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}